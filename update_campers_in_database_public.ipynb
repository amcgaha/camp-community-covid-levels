{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Camper Database\n",
    "\n",
    "Our database stores information on campers, households, and camp sessions across many years. We receive new camper applications every day leading up to summer camp. Before we can compare this season's campers with their local covid data, we need to make sure our database includes recently enrolled campers.\n",
    "\n",
    "For privacy reasons, we update our database with a csv file from my hard drive and don't post personal details publicly. The files originally come from our data management system, called CampMinder. The raw file requires some processing in order to fit into the normalized database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload csv with latest application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {'PersonID' : 'camper_id', \n",
    "              'Application Date': 'application_date', \n",
    "              'Enrolled Sessions': 'session_id', \n",
    "              'Applied Sessions': 'applied_id',\n",
    "              'Full Name': 'name', \n",
    "              'Birth Date': 'birthdate', \n",
    "              'Gender': 'gender', \n",
    "              'Attention Notes': 'notes',\n",
    "              'Primary Childhood ID': 'household_id', \n",
    "              'Primary Childhood HomeAddr1': 'street',\n",
    "              'Primary Childhood HomeCity': 'city', \n",
    "              'Primary Childhood HomeState': 'state',\n",
    "              'Primary Childhood HomeZip': 'zipcode', \n",
    "              'Primary Childhood HomeCountry Name': 'country'}\n",
    "    \n",
    "origin_df = pd.read_csv('C:\\\\Users\\\\avery\\\\OneDrive\\\\covid_geo_docs\\\\latest_campers_for_database.csv')\n",
    "\n",
    "renamed_df = origin_df.rename(column_map, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into three dataframes representing the three applicable tables in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "campers_df = renamed_df[['camper_id', 'name', 'birthdate', 'gender', 'notes', 'household_id']]\n",
    "\n",
    "applications_df = renamed_df[['camper_id', 'application_date', 'session_id', 'applied_id']]\n",
    "\n",
    "households_df = renamed_df[['household_id', 'street', 'city', 'state', 'zipcode', 'country']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare 'campers' dataframe\n",
    "Set camper_id to the index and make birthdate into datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "campers_df['birthdate'] = pd.to_datetime(campers_df['birthdate'])\n",
    "\n",
    "campers_final_df = campers_df.set_index('camper_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare 'households' dataframe\n",
    "\n",
    "First set index to household_id and slice zip codes to first five digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "households_indexed = households_df.set_index('household_id')\n",
    "\n",
    "# Select only first five digits of zip code\n",
    "households_indexed['zipcode'] = households_indexed['zipcode'].astype('str')\n",
    "households_indexed['zipcode'] = households_indexed['zipcode'].str[:5]\n",
    "households_indexed['zipcode'] = households_indexed['zipcode'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate zipcodes are all present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All zip codes present.\n"
     ]
    }
   ],
   "source": [
    "if households_indexed['zipcode'].isna().sum() > 0:\n",
    "    present_text = '!! Missing Zip Codes !!'\n",
    "    print(households_indexed[['household_id', 'zipcode']].sort_values('zipcode', ascending=True).head(5))\n",
    "else:\n",
    "    present_text = 'All zip codes present.'\n",
    "\n",
    "print(present_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate that zipcodes have the right number of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All codes 5 digits.\n"
     ]
    }
   ],
   "source": [
    "digit_text = 'All codes 5 digits.'\n",
    "\n",
    "for value in households_indexed['zipcode']:\n",
    "    if value < 1000:\n",
    "        digit_text = '!! Wrong Digit Present !!'\n",
    "        print(value)\n",
    "        print(households_indexed.loc[value, :])\n",
    "    else:\n",
    "        pass\n",
    "print(digit_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw an error if our zipcodes are off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert present_text == 'All zip codes present.'\n",
    "assert digit_text == 'All codes 5 digits.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename to match later context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "households_final_df = households_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare 'applications' dataframe\n",
    "\n",
    "This one requires transforming the session types from the way they appear in our data management system to the way they're stored in our Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   camper_id application_date                      session_id applied_id\n",
      "0    2424030        9/15/2020              Western Expedition        NaN\n",
      "1    2587669        9/21/2020                       Session 2        NaN\n",
      "2    2601673        10/7/2020  Leadership in Training (LIT) 1        NaN\n",
      "3    2683141        10/1/2020              Western Expedition        NaN\n",
      "4    2748592        9/15/2020              Western Expedition        NaN\n"
     ]
    }
   ],
   "source": [
    "print(applications_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing session_id with applied_id, then drop the applied_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   camper_id application_date                      session_id\n",
      "0    2424030        9/15/2020              Western Expedition\n",
      "1    2587669        9/21/2020                       Session 2\n",
      "2    2601673        10/7/2020  Leadership in Training (LIT) 1\n",
      "3    2683141        10/1/2020              Western Expedition\n",
      "4    2748592        9/15/2020              Western Expedition\n"
     ]
    }
   ],
   "source": [
    "apps_filled = applications_df.fillna(applications_df['applied_id'])\n",
    "apps_fill_only = apps_filled.drop('applied_id', axis=1)\n",
    "\n",
    "print(apps_fill_only.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust index, date type, and string format to match database standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          application_date                      session_id  year_id\n",
      "camper_id                                                          \n",
      "2424030         2020-09-15              western_expedition     2021\n",
      "2587669         2020-09-21                       session_2     2021\n",
      "2601673         2020-10-07  leadership_in_training_(lit)_1     2021\n",
      "2683141         2020-10-01              western_expedition     2021\n",
      "2748592         2020-09-15              western_expedition     2021\n"
     ]
    }
   ],
   "source": [
    "apps_session_format = apps_fill_only.set_index('camper_id')\n",
    "\n",
    "apps_session_format['application_date'] = pd.to_datetime(apps_session_format['application_date'])\n",
    "\n",
    "apps_session_format['session_id'] = apps_session_format['session_id'].replace(' ', '_', regex=True).str.lower()\n",
    "\n",
    "apps_session_format['year_id'] = 2021\n",
    "\n",
    "print(apps_session_format.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Side note: We've been adding new sessions this year, and campers can now choose mulitple sessions. We need to download a list of who's enrolled and what session combinations exist to further refine our database. Let's download a list directly and move on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = pd.DataFrame(apps_session_format.groupby('session_id').count())\n",
    "session_df.to_csv('C:\\\\Users\\\\avery\\\\OneDrive\\\\grp_database_docs\\\\sessions_enrolled_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename to match later context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_final_df = apps_session_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Insert into database\n",
    "\n",
    "Connect to local Postrges database. Remember, the dataframes we want to upload are:\n",
    "\n",
    "campers_final_df = 'campers' \n",
    "\n",
    "applications_final_df = 'applications'\n",
    "\n",
    "households_final_df = 'households'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = '**********'\n",
    "\n",
    "engine = create_engine(f'postgresql://postgres:{password}@localhost:5432/grp_data')\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we update, let's remove values from our dataframes that are already present in the databse tables. We are uploading to a database after all, and the database won't let us upload duplicate values for primary keys. \n",
    "\n",
    "We query the existing database to get a copy of each table as it exists before the update. We'll then merge dataframes, save only unique values, and upload only new data into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   camper_id\n",
      "0    2421760\n",
      "1    2421807\n",
      "2    2421813\n",
      "3    2421814\n",
      "4    2421838\n",
      "camper_id    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "camper_stmt = 'SELECT camper_id FROM campers'\n",
    "campers_db = pd.read_sql(camper_stmt, con=connection, columns=['camper_id'])\n",
    "print(campers_db.head())\n",
    "print(campers_db.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replicate an SQL-style antijoin in Python code, resulting in only values that aren't in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                    object\n",
      "birthdate       datetime64[ns]\n",
      "gender                  object\n",
      "notes                   object\n",
      "household_id             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "join_campers = campers_df['camper_id'].isin(campers_db['camper_id'])\n",
    "\n",
    "anti_join_campers = campers_df[~join_campers].set_index('camper_id')\n",
    "\n",
    "print(anti_join_campers.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also use an antijoin with applications dataframe. The primary key is camper_id and year_id. (We have campers return for multiple years, so we use both columns as the key to identify unique applications.)\n",
    "\n",
    "### We haven't uploaded any apps this year yet, so we don't need to remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apps_stmt = 'SELECT camper_id, year_id FROM applications'\n",
    "apps_db = pd.read_sql(apps_stmt, con=connection, columns=['camper_id', 'year_id'])\n",
    "print(apps_db.head())\n",
    "print(apps_db.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join_apps = applications_df[['camper_id', 'year_id']].isin(apps_db[['camper_id', 'year_id']])\n",
    "\n",
    "anti_join_apps = applications_df[~join_apps].set_index('camper_id')\n",
    "\n",
    "print(anti_join_apps.head())\n",
    "print(anti_join_apps.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do final antijoin with households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   household_id\n",
      "0        929676\n",
      "1       4377270\n",
      "2       2466708\n",
      "3        870609\n",
      "4        657635\n",
      "household_id    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "house_stmt = 'SELECT household_id FROM households'\n",
    "house_db = pd.read_sql(house_stmt, con=connection, columns=['household_id'])\n",
    "print(house_db.head())\n",
    "print(house_db.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "street     object\n",
      "city       object\n",
      "state      object\n",
      "zipcode    object\n",
      "country    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "join_house = households_df['household_id'].isin(house_db['household_id'])\n",
    "\n",
    "anti_join_house = households_df[~join_house].set_index('household_id')\n",
    "\n",
    "print(anti_join_house.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert our dataframes into their respective tables and confirm results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_join_house.to_sql('households', con=connection, if_exists='append')\n",
    "\n",
    "anti_join_campers.to_sql('campers', con=connection, if_exists='append')\n",
    "\n",
    "applications_final_df.to_sql('applications', con=connection, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a query to validate the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender zipcode application_date\n",
      "0  Female   28601       2020-09-15\n",
      "1    Male   37350       2020-09-21\n",
      "2    Male   32746       2020-10-07\n",
      "3  Female   29464       2020-10-01\n",
      "4    Male   27608       2020-09-15\n"
     ]
    }
   ],
   "source": [
    "stmt = \"SELECT c.gender, h.zipcode, a.application_date FROM campers AS c JOIN households AS h USING(household_id) JOIN applications as a USING(camper_id) WHERE a.application_date > '2020-09-01' LIMIT 10;\"\n",
    "\n",
    "result = pd.read_sql(stmt, con=connection)\n",
    "\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Our database is now updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
